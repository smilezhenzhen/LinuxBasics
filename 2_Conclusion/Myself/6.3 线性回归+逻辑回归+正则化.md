---
typora-copy-images-to: assert
---

### 1 多变量线性回归

#### 1.1 多维特征

以房价为例，在原先房间面积的基础上，增加房间数、楼层、使用年限等多个变量，即多维特征来评估房价。

![1564367176026](D:\Git\2_Conclusion\Myself\assets\1564367176026.png)

m代表样本数量，n代表特征数量

![1564367300586](D:\Git\2_Conclusion\Myself\assets\1564367300586.png)代表第i个训练实例，是矩阵中的第i行，是一个向量

![1564367936276](D:\Git\2_Conclusion\Myself\assets\1564367936276.png)代表特征矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征

![1564368072425](D:\Git\2_Conclusion\Myself\assets\1564368072425.png)

#### 1.2 多变量梯度下降

![1564368262761](D:\Git\2_Conclusion\Myself\assets\1564368262761.png)

![1564368271310](D:\Git\2_Conclusion\Myself\assets\1564368271310.png)

![1564368280316](D:\Git\2_Conclusion\Myself\assets\1564368280316.png)

![1564368285968](D:\Git\2_Conclusion\Myself\assets\1564368285968.png)

![1564368310370](D:\Git\2_Conclusion\Myself\assets\1564368310370.png)

![1564368331080](D:\Git\2_Conclusion\Myself\assets\1564368331080.png)

#### 1.3 梯度下降法实践1—特征缩放

![1564368570240](D:\Git\2_Conclusion\Myself\assets\1564368570240.png)

#### 1.4 梯度下降法实践2—学习率

梯度下降算法的每次迭代受到学习率的影响，如果学习率过小，则达到收敛所需的迭代次数会非常高；如果学习率过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

#### 1.5 特征与多项式回归

![1564368863735](D:\Git\2_Conclusion\Myself\assets\1564368863735.png)

如果采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。

#### 1.6 正规方程

正规方程求解向量：![1564369113746](D:\Git\2_Conclusion\Myself\assets\1564369113746.png)

在octave中，正规方程写作：![1564369175242](D:\Git\2_Conclusion\Myself\assets\1564369175242.png)

![1564369214426](D:\Git\2_Conclusion\Myself\assets\1564369214426.png)

![1564369287911](D:\Git\2_Conclusion\Myself\assets\1564369287911.png)

正规方程的python实现：

![1564369323979](D:\Git\2_Conclusion\Myself\assets\1564369323979.png)

#### 1.7 正规方程的不可逆性

pinv函数与inv函数的区别：pinv函数是求矩阵的伪逆，矩阵的逆不存在时也返回有值；inv是求矩阵的逆，不存在时即返回错误。

![1564369936675](D:\Git\2_Conclusion\Myself\assets\1564369936675.png)不可逆的原因：

（1）存在线性相关的特征，即特征向量X是奇异矩阵，可把多余的特征去除

（2）样本数量远远小于特征值的数量

### 2 逻辑回归

#### 2.1 分类问题

在分类问题中，要预测变量y是离散的值，学习一种叫做逻辑回归的算法。在分类问题中，尝试预测的是结果是否属于某一个类（例如正确或者错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；判断肿瘤是恶性还是良性；

二元分类问题：将因变量可能属于的两个类分别称为负向类和正向类，因变量![1564475722520](D:\Git\2_Conclusion\Myself\assets\1564475722520.png)，其中0表示负向类，1表示正向类。

逻辑回归算法的性质是：它的输出值永远在0到1之间，该算法属于分类算法。

#### 2.2 假说表示

逻辑回归模型的假设是：![1564476302009](D:\Git\2_Conclusion\Myself\assets\1564476302009.png)，其中X代表特征向量，g代表逻辑函数，是一个常用的逻辑函数为S形函数，公式为：![1564476433447](D:\Git\2_Conclusion\Myself\assets\1564476433447.png)

![1564476573942](D:\Git\2_Conclusion\Myself\assets\1564476573942.png)

#### 2.3 边界判定

![1564476721603](D:\Git\2_Conclusion\Myself\assets\1564476721603.png)

#### 2.4 代价函数

如何模拟逻辑回归模型的参数θ，即要定义用来拟合参数的优化目标或者代价函数，便是监督学习问题中的逻辑回归模型的拟合问题。

如果沿用线性回归模型的误差平方和的代价函数，得到的代价函数僵尸一个非凸函数，意味着代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。

![1564477340623](D:\Git\2_Conclusion\Myself\assets\1564477340623.png)

![1564477403439](D:\Git\2_Conclusion\Myself\assets\1564477403439.png)

一些梯度下降算法之外的选择：除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加负责和优越，而且通常不需要人工选择学习率，通常比梯度下降算法更加快速。常用的算法有：共轭梯度，局部优化法、有限内存局部优化法。fminuc是matlab和octave中都带的一个最小值优化函数，使用时需要提供代价函数和每个参数的求导。

#### 2.5 简化的成本函数和梯度下降

![1564537047708](D:\Git\2_Conclusion\Myself\assets\1564537047708.png)

逻辑回归与线性回归的更新规则看起来基本相同，但实际假设发生了变化，所以逻辑函数的梯度下降跟线性回归的梯度下降实际上是完全不同的两个算法，我们同样使用监测梯度下降的方法来确保其收敛。当使用梯度下降法来实现逻辑回归时，我们可以使用上述表达式来更新这些参数，也可以使用for循环来更新这些参数值，当然理想情况下是提倡使用向量化来同时更新n个参数。

#### 2.6 高级优化

前面梯度下降的思路是：输入初始参数θ后，计算J(θ)以及J等于0、1直到n时的偏导数项，然后再反复执行这些更新。另外一种梯度下降的思路是：计算J(θ)以及这些偏导数，然后把这些插入到梯度下降中，然后就可以为我们最小化这个函数。优化代价函数的更高级的方法包括共轭梯度法、BFGS(变尺度法)和L-BFGS(限制变尺度法)。

这三个方法的优点包括：不需要手动选择学习率α，因为这些算法的都有一个线性搜索的内部内部循环算法，其可以尝试不同的学习速率α，并且自动选择额一个好的学习速率α，因此其可以甚至可以为每次迭代选择不同的学习速率，就不需要自己选择。

octave有一个非常理想的库用于实现这些先进的优化算法，故可直接调用其自带的库即可得到不错的结果，具体实现如下：

![1564539170667](D:\Git\2_Conclusion\Myself\assets\1564539170667.png)

![1564539235423](D:\Git\2_Conclusion\Myself\assets\1564539235423.png)

#### 2.7 多类别分类：一对多

一对多分类的例子：比如天气包括晴朗，多云，下雨，下雪等分类，因为我们已经知道了二分类问题如何解决，所以对于多分类问题，可以分割成多个二分类问题，即采用“一对余”的方法。

![1564539552511](D:\Git\2_Conclusion\Myself\assets\1564539552511.png)

### 3 正则化

#### 3.1 过拟合问题

![1564542385814](D:\Git\2_Conclusion\Myself\assets\1564542385814.png)

图一的模型是一个线性模型，欠拟合，高偏差，不能很好地适应样本的训练集；图三的模型是一个四次方的模型，过于强调拟合原始数据，丢失了算法预测新数据的本质，过拟合，高方差。若给出一个新的值使之预测，其将表现的很差，预测效果不好；儿图二的模型视乎最合适。

同样的，在分类问题中也存在这样的问题：

![1564542851302](D:\Git\2_Conclusion\Myself\assets\1564542851302.png)

以多项式理解，x的次数越高，拟合的越好，但相应的预测能力就可能变差。那如何解决过拟合问题呢：

1、丢弃一些不能帮助正确预测的特征，可以手工选择保留一些特征，或者使用一些模型选择的算法来帮忙（例如PCA）

2、正则化，保留所有的特征，但是减少参数的大小

#### 3.2 代价函数

![1564543162087](D:\Git\2_Conclusion\Myself\assets\1564543162087.png)

![1564543216016](D:\Git\2_Conclusion\Myself\assets\1564543216016.png)

#### 3.3 正则化线性回归

![1564543298797](D:\Git\2_Conclusion\Myself\assets\1564543298797.png)

正规方程的正则化还能保证求矩阵的逆的那一项是非奇异的，即矩阵的逆一定会存在。

#### 2.4 正则化逻辑回归

![1564543445558](D:\Git\2_Conclusion\Myself\assets\1564543445558.png)

![1564543478219](D:\Git\2_Conclusion\Myself\assets\1564543478219.png)

![1564543519077](D:\Git\2_Conclusion\Myself\assets\1564543519077.png)

