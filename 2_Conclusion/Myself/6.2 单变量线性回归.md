---
typora-copy-images-to: assets
---

### 1 模型表示

以预测房价为例，回归问题的标记如下：

m代表训练集中实例的数量

x代表特征/输入变量

y代表目标变量/输出变量

(x,y)代表训练集中的实例
$$
(x^{(i)},y^{(i)})代表第i个观察实例
$$
h代表学习算法的解决方案或函数也称为假设（hypothesis）

![1562549663993](D:\Git\2_Conclusion\Myself\assets\1562549663993.png)

上图h代表一个函数，输入为房屋尺寸大小，h根据输入x值来得出y值，y值对应房子的价格，h是一个从x到y的函数映射。

![img](file:///D:/Git/2_Conclusion/Myself/assets/Eqn2.gif)

上面的表达式（模型）因为只含有一个特征/输入变量，这样的问题叫作单变量线性回归问题。

### 2 代价函数

为模型选择合适的参数决定得到的直线相对于训练集的准确程度，模型所预测的值与训练集中实际值之间的差距叫作建模误差(modeling error)

目标是选择使得建模误差的平方和能够最小的模型参数，即使得代价函数J最小

![img](file:///D:/Git/2_Conclusion/Myself/assets/Eqn1.gif)

代价函数也叫作平方误差函数，之所以选择误差的平方和，是因为平方代价函数对于大多数问题，特别是回归问题，都是一个合理的选择。

### 3 代价函数的直观理解

![1562551844143](D:\Git\2_Conclusion\Myself\assets\1562551844143.png)

真正需要的是一种有效的算法，能够自动地找出这些使代价函数J取最小值得参数θ_0和θ_1。

### 4 梯度下降

梯度下降是一个用来求函数最小值的算法，即可用梯度下降来求出代价函数J的最小值

梯度下降的思想：开始随机选择一组参数组合计算代价函数，然后寻找下一个能让代价函数值下降最多的参数组合，持续这么做知道到达一个局部最小值。因为没有尝试所有的参数组合，所以不能确定得到的局部最小值便是全局最小值，选择不同的初始参数组合，可能找到不同的局部最小值。

![1562552301908](D:\Git\2_Conclusion\Myself\assets\1562552301908.png)

批量梯度下降算法的公式为：

![1562552345513](D:\Git\2_Conclusion\Myself\assets\1562552345513.png)

其中学习率α是学习率，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。**（注意：必须要同时更新两个参数）**

![1562552510711](D:\Git\2_Conclusion\Myself\assets\1562552510711.png)

### 5 梯度下降的直观理解

![1562552798082](D:\Git\2_Conclusion\Myself\assets\1562552798082.png)

对于θ赋值，使得J(θ)按梯度下降最快的方向进行，一直迭代下去，最终得到局部最小值。其中α是学习率，它决定了沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。

如果α太小，移动太慢，需要很多步才能到达全局最低点；如果α太大，可能一次次越过最低点，最终导致无法收敛甚至发散。随着梯度下降法的进行，移动的幅度回自动变得越来越小，直到最终移动幅度非常小，集已收敛到局部最小值。

![1562553219317](D:\Git\2_Conclusion\Myself\assets\1562553219317.png)

### 6 梯度下降的线性回归

将梯度下降和代价函数结合，应用于具体的拟合直线的线性回归算法里。梯度下降算法和线性回归算法的比较如图：

![1562553386263](D:\Git\2_Conclusion\Myself\assets\1562553386263.png)

对于线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：

![1562553494142](D:\Git\2_Conclusion\Myself\assets\1562553494142.png)

算法改写为：

![1562553520601](D:\Git\2_Conclusion\Myself\assets\1562553520601.png)

